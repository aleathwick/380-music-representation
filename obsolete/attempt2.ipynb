{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"attempt2.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"5X7n18oIHEgx","colab_type":"text"},"source":["# Attempt at something real\n","\n"]},{"cell_type":"code","metadata":{"id":"EGpOj2u4nsMU","colab_type":"code","outputId":"73f21f38-24b7-49a8-d505-ef8de726af8b","executionInfo":{"status":"ok","timestamp":1563448204359,"user_tz":-720,"elapsed":868,"user":{"displayName":"Andrew Leathwick","photoUrl":"","userId":"07830493122468291667"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","fs_path = '/content/drive/My Drive/Study/Machine Learning/firststeps/'"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iN2cjH-6d5bn","colab_type":"code","outputId":"69e5d35e-4617-49ea-9b32-896156a4b7d1","executionInfo":{"status":"ok","timestamp":1563448210594,"user_tz":-720,"elapsed":4165,"user":{"displayName":"Andrew Leathwick","photoUrl":"","userId":"07830493122468291667"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import numpy as np\n","import json\n","import matplotlib.pyplot as plt\n","from keras.models import load_model, Model\n","from keras.layers import Dense, Activation, Dropout, Input, LSTM, Reshape, Lambda, RepeatVector\n","from keras.initializers import glorot_uniform\n","from keras.utils import to_categorical\n","from keras.utils import Sequence\n","from keras.optimizers import Adam\n","from keras import backend as K\n","from keras.callbacks import ModelCheckpoint, TensorBoard\n","import tensorflow as tf"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"4tuGqr_0eDV4","colab_type":"text"},"source":["Set up local :"]},{"cell_type":"code","metadata":{"id":"IFgngqQleb5y","colab_type":"code","outputId":"d315ba91-80e6-4573-c0d1-ef2c9fe2ed06","executionInfo":{"status":"ok","timestamp":1563448228982,"user_tz":-720,"elapsed":7215,"user":{"displayName":"Andrew Leathwick","photoUrl":"","userId":"07830493122468291667"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["with open(fs_path + 'training_data/training_data_completeVAL30.json', 'r') as f:\n","    X, Y, X_val, Y_val, Tx = json.load(f)\n","    \n","print('training data collected')\n","\n","m = len(X)\n","print('m: ', m)\n","Ty = Tx\n","\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["training data collected\n","m:  78518\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NT3zlsynfXbh","colab_type":"text"},"source":["Convert X and Y to arrays:"]},{"cell_type":"code","metadata":{"id":"jbvFSO91kZKZ","colab_type":"code","outputId":"4fc4409a-6695-4631-8c00-8ca8ede009a5","executionInfo":{"status":"ok","timestamp":1563448234769,"user_tz":-720,"elapsed":4531,"user":{"displayName":"Andrew Leathwick","photoUrl":"","userId":"07830493122468291667"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["\n","X = np.array(X)\n","Y = np.array(Y)\n","X_val = np.array(X_val)\n","Y_val = np.array(Y_val)\n","\n","print(\"X dimensions: \", X.shape)\n","print(\"Type of X: \", type(X))\n","print(\"Type of X[0]: \", type(X[0]))\n","print(\"Type of X[0][0]: \", type(X[0][0]))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["X dimensions:  (78518, 256)\n","Type of X:  <class 'numpy.ndarray'>\n","Type of X[0]:  <class 'numpy.ndarray'>\n","Type of X[0][0]:  <class 'numpy.int64'>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WQDoYwshU7bw","colab_type":"code","colab":{}},"source":["class DataGenerator(Sequence):\n","    'Generates data for Keras. This is a subclass of Sequence'\n","    def __init__(self, data, batch_size=16, dim=256, n_channels=1,\n","                 n_classes=333, shuffle=True, n_a = 64):\n","        \"\"\"Initialization\n","        Note that data should be a tuple containing (X, Y)\n","        \"\"\"\n","        self.X_data, self. Y_data = data\n","        self.dim = dim #the dimension of a single example. Should it be (256, 333), the shape of a training example?\n","        self.batch_size = batch_size\n","        self.n_classes = n_classes\n","        self.shuffle = shuffle\n","        self.on_epoch_end()\n","        self.a0 = np.zeros((batch_size, n_a))\n","        self.c0 = np.zeros((batch_size, n_a))\n","\n","    def __len__(self):\n","        'Denotes the number of batches per epoch'\n","        return int(np.floor(len(self.X_data) / self.batch_size))\n","\n","    def __getitem__(self, index):\n","        'Generate one batch of data'\n","        # Generate indexes of the batch\n","        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n","\n","        # Find list of IDs\n","        # list_IDs_temp = [self.list_IDs[k] for k in indexes]\n","\n","        # Generate data\n","        X, Y = self.__data_generation(indexes)\n","        \n","        return [X, self.a0, self.c0], Y\n","\n","    def __data_generation(self, indexes):\n","        'Generates data containing batch_size samples' # X : (n_samples, Tx)\n","        # Initialization\n","        X = np.empty((self.batch_size, self.dim))\n","        Y = np.empty((self.batch_size, self.dim)) #I think this is right...? Because I'll use sparse categorical cross entropy.\n","\n","        # Generate data\n","        for i, index in enumerate(indexes):\n","            # Store sample\n","            X[i,] = self.X_data[index]\n","\n","            # Store expected output\n","            Y[i,] = self.Y_data[index]\n","\n","        return X, list(Y.transpose(1,0))\n","    \n","    def on_epoch_end(self):\n","        'Updates indexes after each epoch'\n","        self.indexes = np.arange(len(self.X_data))\n","        if self.shuffle == True:\n","            np.random.shuffle(self.indexes)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KELLTO7FlP00","colab_type":"text"},"source":["Set up the variables for the model, and the layers"]},{"cell_type":"code","metadata":{"id":"DjhXFrMylSie","colab_type":"code","outputId":"fc2e5e39-1908-42a5-c72f-ebc442f4c5db","executionInfo":{"status":"ok","timestamp":1563448245930,"user_tz":-720,"elapsed":1133,"user":{"displayName":"Andrew Leathwick","photoUrl":"","userId":"07830493122468291667"}},"colab":{"base_uri":"https://localhost:8080/","height":241}},"source":["#number of hidden LSTM states\n","n_a = 256\n","\n","# no of unique events\n","n_values = 333\n","\n","# Some objects we need\n","reshapor = Reshape((1, n_values))\n","reshapor2 = Reshape((1, n_a))\n","LSTM_cell = LSTM(n_a, return_state = True)\n","LSTM_cell2 = LSTM(n_a, return_state = True, batch_input_shape=(1, n_a))\n","# LSTM_cell3 = LSTM(n_a, return_state = True)\n","densor = Dense(n_values, activation='softmax')\n","\n","training_generator = DataGenerator((X, Y), n_a = n_a)\n","# validation_generator = DataGenerator((X_val, Y_val), n_a = n_a)\n","K.tensorflow_backend._get_available_gpus()"],"execution_count":7,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0718 11:10:44.695937 139791366309760 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","W0718 11:10:44.720152 139791366309760 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","W0718 11:10:44.721141 139791366309760 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","W0718 11:10:44.722108 139791366309760 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","W0718 11:10:45.200664 139791366309760 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["['/job:localhost/replica:0/task:0/device:GPU:0']"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"Q5vC-Pzwlfsl","colab_type":"text"},"source":["Here is the model:"]},{"cell_type":"code","metadata":{"id":"K9RGDdzXlg01","colab_type":"code","colab":{}},"source":["def musicmodel_LambdaOneHotLayer(Tx, n_a, n_values):\n","    \"\"\"\n","    Implement the model\n","    \n","    Arguments:\n","    Tx -- length of a sequence in the corpus\n","    n_a -- the number of activations used in our model\n","    n_values -- number of unique values in the music data \n","    \n","    Returns:\n","    model -- a keras model with the \n","    \"\"\"\n","    \n","    # Define the input with a shape \n","    X = Input(shape=(Tx,))\n","    \n","    # Define s0, initial hidden state for the decoder LSTM\n","    a0 = Input(shape=(n_a,), name='a0')\n","    c0 = Input(shape=(n_a,), name='c0')\n","    a = a0\n","    c = c0\n","    a2 = a0\n","    c2 = c0\n","\n","    # Create empty list to append the outputs to while iterating\n","    outputs = []\n","    \n","    # Step 2: Loop\n","    for t in range(Tx):\n","        \n","        # select the \"t\"th time step from X. \n","        x = Lambda(lambda x: x[:,t])(X)\n","        # This will be a float indicating class. But we need the class represented in one hot fashion:\n","        x = Lambda(lambda x: tf.one_hot(K.cast(x, dtype='int32'), 333))(x)\n","        # We then reshape x to be (1, n_values)\n","        x = reshapor(x)\n","        # Perform one step of the LSTM_cell\n","        a, _, c = LSTM_cell(x, initial_state=[a, c])\n","        x = reshapor2(a)\n","        a2, _, c2 = LSTM_cell2(x, initial_state=[a2, c2])\n","        # Apply densor to the hidden state output of LSTM_Cell\n","        out = densor(a2)\n","        # Add the output to \"outputs\"\n","        outputs.append(out)\n","        \n","    # Step 3: Create model instance\n","    model = Model(inputs=[X,a0,c0],outputs=outputs)\n","    \n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XarwGn3AliOP","colab_type":"text"},"source":["To run the model:"]},{"cell_type":"code","metadata":{"id":"IeQz8bIHlpdj","colab_type":"code","outputId":"44e1c5d8-6de1-426e-93b0-2f28734cb270","colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["model = musicmodel_LambdaOneHotLayer(Tx, n_a, n_values)\n","\n","opt = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, decay=0.01)\n","\n","# model.load_weights('weights/model1layer256lstm400epochs.h5')\n","\n","model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","\n","# tb = TensorBoard(log_dir='logs', histogram_freq=0, batch_size=16, write_graph=False, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None, update_freq='epoch')\n","checkpoint = ModelCheckpoint(\"weights/0718weights.2x256.{epoch:02d}-{val_loss:.2f}.hdf5\", monitor='val_loss', verbose=2, save_best_only=True, save_weights_only=True)\n","\n","model.fit_generator(generator=training_generator,\n","                    use_multiprocessing=False, epochs=20, max_queue_size=1, callbacks = [checkpoint])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["W0718 11:12:25.864980 139791366309760 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","W0718 11:12:41.233690 139791366309760 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/20\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XEqZ75WPlqZp","colab_type":"text"},"source":["Here is the inference model:"]},{"cell_type":"code","metadata":{"id":"nobAmCSmlu4l","colab_type":"code","colab":{}},"source":["def music_inference_model(LSTM_cell, densor, n_values, n_a, Ty = 100):\n","    \"\"\"\n","    Uses the trained \"LSTM_cell\" and \"densor\" from model() to generate a sequence of values.\n","    \n","    Arguments:\n","    LSTM_cell -- the trained \"LSTM_cell\" from model(), Keras layer object\n","    densor -- the trained \"densor\" from model(), Keras layer object\n","    n_values -- integer, umber of unique values\n","    n_a -- number of units in the LSTM_cell\n","    Ty -- integer, number of time steps to generate\n","    \n","    Returns:\n","    inference_model -- Keras model instance\n","    \"\"\"\n","    \n","    # Define the input of your model with a shape \n","    x0 = Input(shape=(1, n_values))\n","    \n","    # Define s0, initial hidden state for the decoder LSTM\n","    a0 = Input(shape=(n_a,), name='a0')\n","    c0 = Input(shape=(n_a,), name='c0')\n","    a = a0\n","    c = c0\n","    x = x0\n","\n","    ### START CODE HERE ###\n","    # Step 1: Create an empty list of \"outputs\" to later store your predicted values (≈1 line)\n","    outputs = []\n","    \n","    # Step 2: Loop over Ty and generate a value at every time step\n","    for t in range(Ty):\n","        \n","        # Step 2.A: Perform one step of LSTM_cell (≈1 line)\n","        a, _, c = LSTM_cell(x, initial_state=[a, c])\n","        \n","        # Step 2.B: Apply Dense layer to the hidden state output of the LSTM_cell (≈1 line)\n","        out = densor(a)\n","\n","        # Step 2.C: Append the prediction \"out\" to \"outputs\". out.shape = (None, 78) (≈1 line)\n","        outputs.append(out)\n","        \n","        # Step 2.D: Select the next value according to \"out\", and set \"x\" to be the one-hot representation of the\n","        #           selected value, which will be passed as the input to LSTM_cell on the next step. We have provided \n","        #           the line of code you need to do this. \n","        \n","        x = Lambda(get_max_pred)(x)\n","        # x = tf.one_hot(np.argmax(out), 333)\n","        # x = tf.expand_dims(x, axis=-1)\n","        # x = tf.expand_dims(x, axis=-1)\n","        \n","    # Step 3: Create model instance with the correct \"inputs\" and \"outputs\" (≈1 line)\n","    inference_model = Model(inputs=[x0,a0,c0],outputs=outputs)\n","    \n","    ### END CODE HERE ###\n","    \n","    return inference_model\n","inference_model = music_inference_model(LSTM_cell, densor, n_values, n_a, Ty)\n","\n","x_initializer = np.zeros((1, 1, 333))\n","x_initializer[0][0][308] = 1\n","a_initializer = np.zeros((1, n_a))\n","c_initializer = np.zeros((1, n_a))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EQLoL4Wolvme","colab_type":"text"},"source":["To make predictions:"]},{"cell_type":"code","metadata":{"id":"2xoFjgKvlxIX","colab_type":"code","colab":{}},"source":["def predict_and_sample(inference_model, x_initializer = x_initializer, a_initializer = a_initializer, \n","                       c_initializer = c_initializer):\n","    \"\"\"\n","    Predicts the next value of values using the inference model.\n","    \n","    Arguments:\n","    inference_model -- Keras model instance for inference time\n","    x_initializer -- numpy array of shape (1, 1, 78), one-hot vector initializing the values generation\n","    a_initializer -- numpy array of shape (1, n_a), initializing the hidden state of the LSTM_cell\n","    c_initializer -- numpy array of shape (1, n_a), initializing the cell state of the LSTM_cel\n","    \n","    Returns:\n","    results -- numpy-array of shape (Ty, 256), matrix of one-hot vectors representing the values generated\n","    indices -- numpy-array of shape (Ty, 1), matrix of indices representing the values generated\n","    \"\"\"\n","    \n","    ### START CODE HERE ###\n","    # Step 1: Use your inference model to predict an output sequence given x_initializer, a_initializer and c_initializer.\n","    pred = inference_model.predict([x_initializer,a_initializer,c_initializer])\n","    # Step 2: Convert \"pred\" into an np.array() of indices with the maximum probabilities\n","    #pred is a list of numpy arrays, with probabilities of events at every time step\n","    indices = np.argmax(pred, axis=-1)\n","    # Step 3: Convert indices to one-hot vectors, the shape of the results should be (Ty, n_values)\n","    results = to_categorical(indices, num_classes=n_values)\n","    ### END CODE HERE ###\n","    return results, indices\n","\n","results, indices = predict_and_sample(inference_model, x_initializer, a_initializer, c_initializer)\n","print(\"np.argmax(results[12]) =\", np.argmax(results[12]))\n","print(\"np.argmax(results[17]) =\", np.argmax(results[17]))\n","print(\"list(indices[12:18]) =\", list(indices[12:18]))\n","\n","dump_pickle_data((results, indices), '4_epochs_try.pkl')\n","\n","print(callbacks_list)"],"execution_count":0,"outputs":[]}]}