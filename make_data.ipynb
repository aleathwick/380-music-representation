{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pretty_midi\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import my methods\n",
    "from modules.midiMethods import *\n",
    "from modules.dataMethods import *\n",
    "import modules.models as models\n",
    "import modules.mlClasses as mlClasses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "The code blocks below generate X sequences only, that is, thay have no target sequences - that is because the targets are simply the original sequence moved along by a single time step, resulting in much redundant storage if these are stored separately.\n",
    "\n",
    "The following code blocks are not comprehensive, but provide some examples of how to use my methods to generate training data, or chroma data from training data.\n",
    "\n",
    "### Read in information about where data is stored\n",
    "The following reads in the filenames and locations of the train and validation partitions of the MAESTRO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maestro = pd.read_csv('training_data/maestro-v2.0.0.csv', index_col=0)\n",
    "filenames_train = list(maestro[maestro['split'] == 'train']['midi_filename'])\n",
    "filenames_val = list(maestro[maestro['split'] == 'validation']['midi_filename'])\n",
    "data_path = 'training_data/MaestroV2.00/maestro-v2.0.0/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Performance Representation Data\n",
    "Throughout this project, I referred to Performance Representation as oore. The following produces training and validation examples of the second version of oore that I used, which represented velocity and time shifts at lower resolution than the original paper, using 20ms increments for time shifts, and 16 possible velocity values.\n",
    "Training examples are produced at three speeds. Harmonic augmentation is not produced as separate data, processed on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for oore, get 601 so that we can use 600 at train time\n",
    "# (Maybe I corrected in the function for this already!)\n",
    "for speed in [0.9, 1, 1.1]:\n",
    "    X = get_processed_oore2_data(data_path, filenames_train, skip=1, n_events=601, speed=speed)\n",
    "    print('examples in X: ', len(X))\n",
    "    with open(f'training_data/oore_v2/oore2_train_{speed}.json', 'w') as f:\n",
    "        json.dump(X, f)\n",
    "\n",
    "X_val = get_processed_oore2_data(data_path, filenames_val, skip=1, n_events=601, speed=speed)\n",
    "print('examples in X: ', len(X))\n",
    "with open(f'training_data/oore_v2/oore2_val.json', 'w') as f:\n",
    "    json.dump(X_val, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating chroma for NoteTuple data\n",
    "The first code block generates note bin data (called NoteTuple in the original paper in which it was introduced), whilst the second generates the corresponding chroma data.\n",
    "\n",
    "Chroma takes up quite a bit of space, especially with needing three different versions for different speeds. Ideally the speed data augmentation would take place as the data was being fed into the model, but there is a potential bottleneck there, and that would result in only one version of chroma needed. In fact, there is nothing stopping the same chroma data being used for different speeds - it would just require some reorganizing of how I store and get the examples. The same is not true for oore data, in which sequence length changes with speed, due to differing numbers of tokens needed to represent longer timeshifts of different lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for speed in [0.90,1.1]:\n",
    "    X = files2note_bin_examples(data_path, filenames_train, skip=1, n_notes=220, speed=speed)\n",
    "    with open(f'training_data/note_bin_v2/nb_220_train{speed}.json', 'w') as f:\n",
    "        json.dump(X, f)\n",
    "\n",
    "X_val = files2note_bin_examples(data_path, filenames_train, skip=1, n_notes=220, speed=1)\n",
    "with open(f'training_data/note_bin_v2/nb_220_val.json', 'w') as f:\n",
    "    json.dump(X_val, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'training_data/note_bin_v2/'\n",
    "\n",
    "for speed in [0.9, 1, 1.1]:\n",
    "    # chroma modes to make data for\n",
    "    for mode in ['weighted', 'normal', 'lowest']:\n",
    "        with open(data_path + f'nb_220_train{speed}.json', 'r') as f:\n",
    "            examples = json.load(f)\n",
    "        print(len(examples[0]))\n",
    "        chroma = nb_data2chroma(np.array(examples),  mode=mode)\n",
    "        with open(data_path + f'nb_220_train{speed}_chroma{mode}.json', 'w') as f:\n",
    "            json.dump(chroma.tolist(), f)\n",
    "\n",
    "for mode in ['weighted', 'normal', 'lowest']:\n",
    "    with open(data_path + f'nb_220_val.json', 'r') as f:\n",
    "                val = json.load(f)\n",
    "    chroma_val = nb_data2chroma(np.array(val),  mode=mode)\n",
    "    with open(data_path + f'nb_220_val_{mode}.json', 'w') as f:\n",
    "        json.dump(chroma_val.tolist(), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
